{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "In this example a bayesian framework is defined to tune hyperparameter of a CNN using hyperopt library developed https://github.com/hyperopt\n",
    "Bayesian optimization is a seuential model-based approach to solving problems. In particular, we prescribe a prior belief over the possible objective functions and then sequentially refine this model as data are observed via our updated beliefs-given data-on the likely ojective function we are optimizing.\n",
    "https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf\n",
    "This blog summarises bayesian optimization very thoroughly.\n",
    "https://medium.com/vantageai/bringing-back-the-time-spent-on-hyperparameter-tuning-with-bayesian-optimisation-2e21a3198afb\n",
    "\n",
    "The CNN is used to model fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading fashion MNIST dataset and defining CNN\n",
    "In this section we load the fashion mnist dataset which contains 10 classes of objects, and a CNN is defined for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline cnn model for fashion mnist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "\t# reshape dataset to have a single channel\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t# one hot encode target values\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(params):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(filters = int(params['Kernels1']), kernel_size= (int(params['kernel_size1']),int(params['kernel_size1'])), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(units = int(params['units1']), activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=params['LR'], momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(Model, dataX, dataY, n_folds):\n",
    "\tscores, histories = list(), list()\n",
    "\t# prepare cross validation\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t# enumerate splits\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\n",
    "\t\t# select rows for train and test\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t# fit model\n",
    "\t\thistory = Model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t# evaluate model\n",
    "\t\t_, acc = Model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t# append scores\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "Here the hyper parameter grid is defined to be used by the bayesian framework. They will be jointly discovered as optimum design choice. This framework has two main ingredients:\n",
    "### 1) Probabilistic surrogate model:\n",
    "This model consists of a prior distribution that captures the behaviour of the unknown objective function from the designer perspective, and an observation model that describes the data generation.\n",
    "### 2) Loss function\n",
    "It is a sequence of queries to be minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyper parameter space\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "#%% Hyper parameter tuning\n",
    "param_hyperopt = {\n",
    "    'LR': hp.loguniform('lr', np.log(0.01), np.log(1)),\n",
    "    'Kernels1': hp.uniform('Kernels1', 8, 32),\n",
    "    'kernel_size1': hp.quniform('kernel_size', 3, 5, 3),\n",
    "    'units1': hp.quniform('units', 50, 100, 4)\n",
    "    }\n",
    "param_space = param_hyperopt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting up the hyperopt\n",
    "After defining the parameter space, the hyperopt need to be initialized. One need to consider that the bayesian process always maximize the posteriori, therefore if one is after minimizing a function that function could be defined as (1 - of interest object). The maximization of the posteriori is done iteratively through the below process: \n",
    "\n",
    "For each iteration:\n",
    "\n",
    "1) Find the set of hyperparameter values that maximise the Expected Improvement by optimising the selection function over the surrogate function.\n",
    "\n",
    "2) Hand this hyperparameter combination to the objective function for evaluation — and retrieve the corresponding score.\n",
    "\n",
    "3) Update the surrogate function along the feedback of the objective function by applying Bayes’ theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Hyperopt function for CNN fashion MNIST\n",
    "def hyperopt(param_space, X_train, y_train, X_test, y_test, num_eval):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    def objective_function(params):\n",
    "        clf = define_model(params)\n",
    "        scores, histories = evaluate_model(clf, trainX, trainY, n_folds = 5)\n",
    "        score = np.mean(scores)\n",
    "        return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_param = fmin(objective_function, \n",
    "                      param_space, \n",
    "                      algo=tpe.suggest, \n",
    "                      max_evals=num_eval, \n",
    "                      trials=trials,\n",
    "                      rstate= np.random.RandomState(1))\n",
    "    loss = [x['result']['loss'] for x in trials.trials]\n",
    "    \n",
    "    best_param_values = [x for x in best_param.values()]\n",
    "    \n",
    "    best_param_dic = {'LR' : best_param_values[2],\n",
    "                     'Kernels1' : best_param_values[0],\n",
    "                     'kernel_size1' : best_param_values[1],\n",
    "                     'units1' : best_param_values[3]}\n",
    "    \n",
    "    clf_best = define_model(best_param_dic)\n",
    "    \n",
    "    clf_best.fit(trainX, trainY)\n",
    "    Y_pred = clf_best.predict(testX)\n",
    "    Y_pred = np.rint(Y_pred)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy_score(testY, Y_pred)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"##### Results\")\n",
    "    print(\"Score best parameters: \", min(loss)*-1)\n",
    "    print(\"Best parameters: \", best_param)\n",
    "    print(\"Test Score: \", accuracy_score(testY, Y_pred))\n",
    "    print(\"Time elapsed: \", time.time() - start)\n",
    "    print(\"Parameter combinations evaluated: \", num_eval)\n",
    "    \n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 10.267                                            \n",
      "> 10.167                                            \n",
      "> 10.525                                            \n",
      "> 10.225                                            \n",
      "> 10.483                                            \n",
      "> 90.308                                                                         \n",
      "> 93.875                                                                         \n",
      "> 96.217                                                                         \n",
      "> 96.808                                                                         \n",
      "> 99.067                                                                         \n",
      "> 89.025                                                                         \n",
      "> 92.358                                                                         \n",
      "> 94.133                                                                         \n",
      "> 94.692                                                                         \n",
      "> 95.942                                                                         \n",
      "> 89.542                                                                         \n",
      "> 92.633                                                                         \n",
      "> 95.267                                                                         \n",
      "> 97.342                                                                         \n",
      "> 98.708                                                                         \n",
      "> 88.825                                                                         \n",
      "> 90.883                                                                         \n",
      "> 91.158                                                                         \n",
      "> 91.858                                                                         \n",
      "> 91.350                                                                         \n",
      "> 89.275                                                                         \n",
      "> 92.533                                                                         \n",
      "> 92.350                                                                         \n",
      "> 95.967                                                                         \n",
      "> 94.625                                                                         \n",
      "> 88.042                                                                         \n",
      "> 87.783                                                                         \n",
      "> 90.200                                                                         \n",
      "> 89.567                                                                         \n",
      "> 90.158                                                                         \n",
      "> 89.775                                                                         \n",
      "> 92.975                                                                       \n",
      "> 95.867                                                                       \n",
      "> 97.008                                                                       \n",
      "> 98.417                                                                       \n",
      "> 10.275                                                                       \n",
      "> 9.958                                                                        \n",
      "> 10.025                                                                       \n",
      "> 10.125                                                                       \n",
      "> 9.908                                                                        \n",
      "> 83.333                                                                       \n",
      "> 84.042                                                                       \n",
      "> 82.233                                                                         \n",
      "> 68.075                                                                         \n",
      "> 9.833                                                                          \n",
      "> 10.275                                                                          \n",
      "> 10.275                                                                          \n",
      "> 9.858                                                                           \n",
      "> 10.033                                                                          \n",
      "> 9.983                                                                           \n",
      "> 10.142                                                                          \n",
      "> 9.958                                                                           \n",
      "> 10.000                                                                          \n",
      "> 10.017                                                                          \n",
      "> 10.217                                                                          \n",
      "> 9.658                                                                           \n",
      "> 9.917                                                                           \n",
      "> 9.858                                                                           \n",
      "> 9.992                                                                           \n",
      "> 9.983                                                                           \n",
      "> 89.475                                                                          \n",
      "> 90.608                                                                          \n",
      "> 93.008                                                                          \n",
      "> 94.083                                                                          \n",
      "> 94.717                                                                          \n",
      "> 85.600                                                                          \n",
      "> 86.383                                                                          \n",
      "> 86.983                                                                          \n",
      "> 85.333                                                                          \n",
      "> 87.000                                                                          \n",
      "100%|██████████| 15/15 [1:37:30<00:00, 379.99s/it, best loss: -0.9525500059127807]\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - ETA: 1:25 - loss: 2.3970 - accuracy: 0.09 - ETA: 13s - loss: 1.8450 - accuracy: 0.3036 - ETA: 10s - loss: 1.5381 - accuracy: 0.436 - ETA: 9s - loss: 1.3366 - accuracy: 0.510 - ETA: 9s - loss: 1.2109 - accuracy: 0.55 - ETA: 8s - loss: 1.1083 - accuracy: 0.59 - ETA: 8s - loss: 1.0389 - accuracy: 0.62 - ETA: 8s - loss: 0.9779 - accuracy: 0.64 - ETA: 8s - loss: 0.9316 - accuracy: 0.66 - ETA: 7s - loss: 0.9092 - accuracy: 0.67 - ETA: 7s - loss: 0.8841 - accuracy: 0.67 - ETA: 7s - loss: 0.8548 - accuracy: 0.68 - ETA: 7s - loss: 0.8336 - accuracy: 0.69 - ETA: 7s - loss: 0.8145 - accuracy: 0.70 - ETA: 7s - loss: 0.7980 - accuracy: 0.70 - ETA: 7s - loss: 0.7846 - accuracy: 0.71 - ETA: 7s - loss: 0.7725 - accuracy: 0.71 - ETA: 7s - loss: 0.7598 - accuracy: 0.71 - ETA: 7s - loss: 0.7449 - accuracy: 0.72 - ETA: 7s - loss: 0.7302 - accuracy: 0.73 - ETA: 7s - loss: 0.7176 - accuracy: 0.73 - ETA: 7s - loss: 0.7091 - accuracy: 0.73 - ETA: 7s - loss: 0.6991 - accuracy: 0.74 - ETA: 7s - loss: 0.6912 - accuracy: 0.74 - ETA: 7s - loss: 0.6836 - accuracy: 0.74 - ETA: 7s - loss: 0.6759 - accuracy: 0.74 - ETA: 6s - loss: 0.6688 - accuracy: 0.75 - ETA: 6s - loss: 0.6645 - accuracy: 0.75 - ETA: 6s - loss: 0.6564 - accuracy: 0.75 - ETA: 6s - loss: 0.6504 - accuracy: 0.75 - ETA: 6s - loss: 0.6431 - accuracy: 0.76 - ETA: 6s - loss: 0.6376 - accuracy: 0.76 - ETA: 6s - loss: 0.6369 - accuracy: 0.76 - ETA: 6s - loss: 0.6353 - accuracy: 0.76 - ETA: 6s - loss: 0.6289 - accuracy: 0.76 - ETA: 6s - loss: 0.6226 - accuracy: 0.77 - ETA: 6s - loss: 0.6180 - accuracy: 0.77 - ETA: 6s - loss: 0.6131 - accuracy: 0.77 - ETA: 6s - loss: 0.6080 - accuracy: 0.77 - ETA: 6s - loss: 0.6045 - accuracy: 0.77 - ETA: 5s - loss: 0.6007 - accuracy: 0.77 - ETA: 5s - loss: 0.5968 - accuracy: 0.78 - ETA: 5s - loss: 0.5941 - accuracy: 0.78 - ETA: 5s - loss: 0.5893 - accuracy: 0.78 - ETA: 5s - loss: 0.5850 - accuracy: 0.78 - ETA: 5s - loss: 0.5808 - accuracy: 0.78 - ETA: 5s - loss: 0.5757 - accuracy: 0.79 - ETA: 5s - loss: 0.5728 - accuracy: 0.79 - ETA: 5s - loss: 0.5688 - accuracy: 0.79 - ETA: 5s - loss: 0.5664 - accuracy: 0.79 - ETA: 5s - loss: 0.5627 - accuracy: 0.79 - ETA: 5s - loss: 0.5604 - accuracy: 0.79 - ETA: 5s - loss: 0.5579 - accuracy: 0.79 - ETA: 5s - loss: 0.5561 - accuracy: 0.79 - ETA: 5s - loss: 0.5528 - accuracy: 0.80 - ETA: 4s - loss: 0.5498 - accuracy: 0.80 - ETA: 4s - loss: 0.5463 - accuracy: 0.80 - ETA: 4s - loss: 0.5433 - accuracy: 0.80 - ETA: 4s - loss: 0.5411 - accuracy: 0.80 - ETA: 4s - loss: 0.5385 - accuracy: 0.80 - ETA: 4s - loss: 0.5354 - accuracy: 0.80 - ETA: 4s - loss: 0.5344 - accuracy: 0.80 - ETA: 4s - loss: 0.5327 - accuracy: 0.80 - ETA: 4s - loss: 0.5316 - accuracy: 0.80 - ETA: 4s - loss: 0.5291 - accuracy: 0.80 - ETA: 4s - loss: 0.5269 - accuracy: 0.81 - ETA: 4s - loss: 0.5251 - accuracy: 0.81 - ETA: 4s - loss: 0.5239 - accuracy: 0.81 - ETA: 4s - loss: 0.5213 - accuracy: 0.81 - ETA: 4s - loss: 0.5198 - accuracy: 0.81 - ETA: 4s - loss: 0.5184 - accuracy: 0.81 - ETA: 4s - loss: 0.5174 - accuracy: 0.81 - ETA: 3s - loss: 0.5152 - accuracy: 0.81 - ETA: 3s - loss: 0.5131 - accuracy: 0.81 - ETA: 3s - loss: 0.5101 - accuracy: 0.81 - ETA: 3s - loss: 0.5090 - accuracy: 0.81 - ETA: 3s - loss: 0.5085 - accuracy: 0.81 - ETA: 3s - loss: 0.5068 - accuracy: 0.81 - ETA: 3s - loss: 0.5056 - accuracy: 0.81 - ETA: 3s - loss: 0.5034 - accuracy: 0.81 - ETA: 3s - loss: 0.5013 - accuracy: 0.81 - ETA: 3s - loss: 0.5004 - accuracy: 0.82 - ETA: 3s - loss: 0.4990 - accuracy: 0.82 - ETA: 3s - loss: 0.4977 - accuracy: 0.82 - ETA: 3s - loss: 0.4958 - accuracy: 0.82 - ETA: 3s - loss: 0.4951 - accuracy: 0.82 - ETA: 3s - loss: 0.4934 - accuracy: 0.82 - ETA: 3s - loss: 0.4916 - accuracy: 0.82 - ETA: 3s - loss: 0.4911 - accuracy: 0.82 - ETA: 3s - loss: 0.4905 - accuracy: 0.82 - ETA: 2s - loss: 0.4897 - accuracy: 0.82 - ETA: 2s - loss: 0.4889 - accuracy: 0.82 - ETA: 2s - loss: 0.4882 - accuracy: 0.82 - ETA: 2s - loss: 0.4869 - accuracy: 0.82 - ETA: 2s - loss: 0.4866 - accuracy: 0.82 - ETA: 2s - loss: 0.4844 - accuracy: 0.82 - ETA: 2s - loss: 0.4832 - accuracy: 0.82 - ETA: 2s - loss: 0.4817 - accuracy: 0.82 - ETA: 2s - loss: 0.4799 - accuracy: 0.82 - ETA: 2s - loss: 0.4783 - accuracy: 0.82 - ETA: 2s - loss: 0.4772 - accuracy: 0.82 - ETA: 2s - loss: 0.4756 - accuracy: 0.82 - ETA: 2s - loss: 0.4746 - accuracy: 0.82 - ETA: 2s - loss: 0.4732 - accuracy: 0.82 - ETA: 2s - loss: 0.4715 - accuracy: 0.83 - ETA: 2s - loss: 0.4702 - accuracy: 0.83 - ETA: 2s - loss: 0.4683 - accuracy: 0.83 - ETA: 2s - loss: 0.4674 - accuracy: 0.83 - ETA: 2s - loss: 0.4661 - accuracy: 0.83 - ETA: 1s - loss: 0.4645 - accuracy: 0.83 - ETA: 1s - loss: 0.4626 - accuracy: 0.83 - ETA: 1s - loss: 0.4619 - accuracy: 0.83 - ETA: 1s - loss: 0.4612 - accuracy: 0.83 - ETA: 1s - loss: 0.4598 - accuracy: 0.83 - ETA: 1s - loss: 0.4585 - accuracy: 0.83 - ETA: 1s - loss: 0.4572 - accuracy: 0.83 - ETA: 1s - loss: 0.4563 - accuracy: 0.83 - ETA: 1s - loss: 0.4553 - accuracy: 0.83 - ETA: 1s - loss: 0.4547 - accuracy: 0.83 - ETA: 1s - loss: 0.4539 - accuracy: 0.83 - ETA: 1s - loss: 0.4528 - accuracy: 0.83 - ETA: 1s - loss: 0.4526 - accuracy: 0.83 - ETA: 1s - loss: 0.4520 - accuracy: 0.83 - ETA: 1s - loss: 0.4514 - accuracy: 0.83 - ETA: 1s - loss: 0.4511 - accuracy: 0.83 - ETA: 1s - loss: 0.4507 - accuracy: 0.83 - ETA: 1s - loss: 0.4493 - accuracy: 0.83 - ETA: 0s - loss: 0.4483 - accuracy: 0.83 - ETA: 0s - loss: 0.4475 - accuracy: 0.83 - ETA: 0s - loss: 0.4464 - accuracy: 0.83 - ETA: 0s - loss: 0.4450 - accuracy: 0.83 - ETA: 0s - loss: 0.4443 - accuracy: 0.84 - ETA: 0s - loss: 0.4436 - accuracy: 0.84 - ETA: 0s - loss: 0.4424 - accuracy: 0.84 - ETA: 0s - loss: 0.4415 - accuracy: 0.84 - ETA: 0s - loss: 0.4405 - accuracy: 0.84 - ETA: 0s - loss: 0.4398 - accuracy: 0.84 - ETA: 0s - loss: 0.4393 - accuracy: 0.84 - ETA: 0s - loss: 0.4384 - accuracy: 0.84 - ETA: 0s - loss: 0.4377 - accuracy: 0.84 - ETA: 0s - loss: 0.4371 - accuracy: 0.84 - ETA: 0s - loss: 0.4367 - accuracy: 0.84 - ETA: 0s - loss: 0.4360 - accuracy: 0.84 - ETA: 0s - loss: 0.4357 - accuracy: 0.84 - ETA: 0s - loss: 0.4347 - accuracy: 0.84 - ETA: 0s - loss: 0.4341 - accuracy: 0.84 - 8s 127us/step - loss: 0.4339 - accuracy: 0.8439\n",
      "\n",
      "##### Results\n",
      "Score best parameters:  0.9525500059127807\n",
      "Best parameters:  {'Kernels1': 11.495441489540417, 'kernel_size': 3.0, 'lr': 0.013283222998324686, 'units': 84.0}\n",
      "Test Score:  0.8326\n",
      "Time elapsed:  5858.2807738780975\n",
      "Parameter combinations evaluated:  15\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = load_dataset()\n",
    "# prepare pixel data\n",
    "trainX, testX = prep_pixels(trainX, testX)\n",
    "# Hyper parameter tuning\n",
    "num_eval = 15\n",
    "results_hyperopt = hyperopt(param_hyperopt, trainX, trainY, testX, testY, num_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10333333 0.95255001 0.95255001 0.95255001 0.95255001 0.95255001\n",
      " 0.95255001 0.95255001 0.95255001 0.95255001 0.95255001 0.95255001\n",
      " 0.95255001 0.95255001 0.95255001]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Hyperopt scores\n",
    "\n",
    "hyperopt_scores = [trial['result']['loss']*-1 for trial in results_hyperopt.trials]\n",
    "hyperopt_scores = np.maximum.accumulate(hyperopt_scores)\n",
    "print(hyperopt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
